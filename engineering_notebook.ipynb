{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "A Hidden Markov Model (HMM) is a statistical model that represents a system composed of a series of unobserved (hidden) states. These models are widely used in areas such as speech recognition, bioinformatics, and time series analysis (the one we will be doing).\n",
    "\n",
    "## Key Components\n",
    "\n",
    "An HMM is characterized by the following components:\n",
    "\n",
    "- **States**: A finite set of hidden states, usually denoted as $S = {s_1, s_2, \\ldots, s_N}$..\n",
    "- **Observations**: A sequence of observations, which are typically denoted as $O = {o_1, o_2, \\ldots, o_T}$, where each observation is related to a state.\n",
    "- **Transition Probabilities**: The probability of transitioning from one state to another, represented by the matrix $A = [a_{ij}]$, where $a_{ij}$ is the probability of moving from state $i$ to state $j$ and $a_{ii}$ represents the probability of remaining in state $i$.\n",
    "- **Emission/Output Probabilities**: The probability of an observation being generated from a state, represented by the matrix  $B = [b_{ij}]$, where $b_{ij}$ is the probability of observing $o_j$ from state $i$.\n",
    "- **Initial State Probabilities**: The initial probability distribution across states, represented by the vector $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_N)$, where $\\pi_i$, is the probability that the Markov chain will start in state  state $i$..\n",
    "\n",
    "## Basic Assumptions\n",
    "\n",
    "HMMs operate under two key assumptions:\n",
    "\n",
    "1. **Markov Assumption**: The probability of a state only depends on the immediate previous state. In other words  $\\text P(\\text{t}_{i}|{t_1}...\\text{t}_{i-1}) = P(\\text{t}_{i}|\\text{t}_{i-1})$\n",
    "\n",
    "## How it works\n",
    "\n",
    "1. Given the transition matrix $A$ and emission vector $B$, we want to find $P(O| \\pi, P)$ where $O$ represents sequence of observations $O = {o_1, o_2, \\ldots, o_T}$\n",
    "\n",
    "2. To calculate the probability, we must find all possible combination of states using the formula: $\\text{Likelihood(Sequence)} = P(\\text{State}_1) \\times P(\\text{State}_2 | \\text{State}_1) \\times P(\\text{State}_3 | \\text{State}_1, \\text{State}_2) \\times \\ldots \\times P(\\text{State}_T | \\text{State}_1, \\text{State}_2, \\ldots, \\text{State}_{T-1})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I'm doing/Action Items\n",
    "- Look at data/website and see which ones are good and how to use them.\n",
    "- Look for specific libraries in python for markov chains (hidden markov chains).\n",
    "    - Understand the data structure we are going to create.\n",
    "- Research the structures and rules for NFL play calling.\n",
    "- Long Short Term Memory (LSTM) for longer sequences.\n",
    "- Maybe this isn't even a sequence based problem where observations are independent and we can use a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
