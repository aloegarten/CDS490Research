{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I'm doing/Action Items\n",
    "- Look at data/website and see which ones are good and how to use them.\n",
    "- Look for specific libraries in python for markov chains (hidden markov chains).\n",
    "    - Understand the data structure we are going to create.\n",
    "- Research the structures and rules for NFL play calling.\n",
    "- Long Short Term Memory (LSTM) for longer sequences.\n",
    "- Maybe this isn't even a sequence based problem where observations are independent and we can use a neural network\n",
    "- Research the algorithims needed\n",
    "- Dr. Romanelli look at https://web.stanford.edu/~jurafsky/slp3/A.pdf\n",
    "- Find simple example of implementation\n",
    "- Evaluate example in the viterbi paper for inital test case in program\n",
    "- Examine run and pass probabilities per down in this paper: https://mymasonportal.gmu.edu/ultra/courses/_513215_1/cl/outline\n",
    "- https://mymasonportal.gmu.edu/ultra/courses/_513215_1/cl/outline examine this paper\n",
    "- Read Predicting play calls in the National Football League using hidden Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Sources of Data\n",
    "- https://www.nflfastr.com/\n",
    "- https://nflsavant.com/search.php\n",
    "\n",
    "## Possible Libraries\n",
    "- https://github.com/hmmlearn/hmmlearn?tab=readme-ov-file\n",
    "- https://hmmlearn.readthedocs.io/en/stable/api.html#hmmlearn.hmm.CategoricalHMM\n",
    "    - Specifically the categorical model\n",
    "\n",
    "\n",
    "## Sources:\n",
    "- https://web.stanford.edu/~jurafsky/slp3/A.pdf\n",
    "- https://www.cis.upenn.edu/~cis2620/notes/Example-Viterbi-DNA.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "A Hidden Markov Model (HMM) is a statistical model that represents a system composed of a series of unobserved (hidden) states. These models are widely used in areas such as speech recognition, bioinformatics, and time series analysis.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "An HMM is characterized by the following components:\n",
    "\n",
    "- **States**: A finite set of hidden states, usually denoted as $S = \\{s_1, s_2, \\ldots, s_N\\}$.\n",
    "- **Observations**: A sequence of observations, typically denoted as $O = \\{o_1, o_2, \\ldots, o_T\\}$, where each observation is related to a state.\n",
    "- **Transition Probabilities**: The probability of transitioning from one state to another, represented by the matrix $A = [a_{ij}]$, where $a_{ij}$ is the probability of moving from state $i$ to state $j$.\n",
    "- **Emission Probabilities**: The probability of an observation being generated from a state, represented by the matrix $B = [b_{ij}]$, where $b_{ij}$ is the probability of observing $o_j$ from state $i$.\n",
    "- **Initial State Probabilities**: The initial probability distribution across states, represented by the vector $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_N)$.\n",
    "\n",
    "## Basic Assumptions\n",
    "\n",
    "HMMs operate under two key assumptions:\n",
    "\n",
    "1. **Markov Assumption**: The probability of a state only depends on the immediate previous state. That is, $P(t_{i}|t_1, \\ldots, t_{i-1}) = P(t_{i}|t_{i-1})$.\n",
    "\n",
    "## How it Works\n",
    "\n",
    "1. Given the transition matrix $A$ and emission matrix $B$, we want to find $P(O| \\pi, A, B)$ where $O$ represents the sequence of observations $O = \\{o_1, o_2, \\ldots, o_T\\}$.\n",
    "\n",
    "2. To calculate the probability, we must find all possible combinations of states using the formula:\n",
    "   $$\\text{Likelihood(Sequence)} = P(\\text{State}_1) \\times P(\\text{State}_2 | \\text{State}_1) \\times \\ldots \\times P(\\text{State}_T | \\text{State}_{T-1})$$\n",
    "\n",
    "3. However, due to the Markov assumption, the probability of a state at time $t$ depends only on the state at time $t-1$. Therefore, the calculation simplifies to:\n",
    "   $$\\text{Likelihood(Sequence)} = P(\\text{State}_1) \\times P(\\text{State}_2 | \\text{State}_1) \\times \\ldots \\times P(\\text{State}_T | \\text{State}_{T-1})$$\n",
    "\n",
    "4. Additionally, each state generates an observation. The likelihood of the observation sequence given the state sequence is computed using the emission probabilities from matrix $B$.\n",
    "\n",
    "5. The overall probability of the observation sequence $O$ given the model parameters ($A$, $B$, and $\\pi$) is then the sum of the probabilities of observing $O$ across all possible state sequences. This is typically computed using a dynamic programming algorithm such as the Forward algorithm.\n",
    "\n",
    "6. There are three fundamental problems for HMMs:\n",
    "   - **Evaluation Problem**: Determine the likelihood of an observation sequence given the HMM parameters. Solved using the Forward or Backward algorithm.\n",
    "   - **Decoding Problem**: Find the most likely sequence of hidden states given the observation sequence and HMM parameters. The Viterbi algorithm is used for this purpose.\n",
    "   - **Learning Problem**: Learn the HMM parameters given an observation sequence and the set of states. This is typically done using the Baum-Welch algorithm or other forms of Expectation-Maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
